{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep learning for knowledge graphs from scratch",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unmeshvrije/Pytensorflow/blob/master/Deep_learning_for_knowledge_graphs_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ4ycTX8SvjZ",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning for Knowledge Graphs from scratch\n",
        "\n",
        "slides: https://bit.ly/2PIcGez\n",
        "\n",
        "This notebook shows some of the basics of deep learning in pytorch for machine learning with knowledge graphs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j4gC0Kl45A-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we will be building everything from scratch on top of the pytorch framework\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAJafF9etRRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# these libraries aren't installed by default in colab. The exclamation point\n",
        "# executes a command line statement\n",
        "!pip install rdflib wget"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5br7RV5y5Iel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility functions for later. Execute this cell, but ignore what's in it.\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def generate_likes(num_users, num_items):\n",
        "  \"\"\"\n",
        "  Generate fake user/item data\n",
        "  \"\"\"\n",
        "\n",
        "  users = torch.randn(num_users, 2)\n",
        "  items = torch.randn(num_items, 2)\n",
        "\n",
        "  return (torch.mm(users, items.t()) > 0.7).nonzero()\n",
        "\n",
        "def load_aifb():\n",
        "    \"\"\"\n",
        "    Loads a knowledge graph dataset.\n",
        "    \n",
        "    :return: A tuple containing the graph data, and the classification test and train sets.\n",
        "    \"\"\"\n",
        "    online = 'https://github.com/pbloem/gated-rgcn/blob/master/data/aifb/aifb_stripped.nt.gz?raw=true'\n",
        "    file = 'aifb.nt.gz'\n",
        "\n",
        "    wget.download(online, out=file) # download the file from github\n",
        "\n",
        "    \n",
        "    # -- Parse the data with RDFLib\n",
        "    graph = rdf.Graph()\n",
        "\n",
        "    if file.endswith('nt.gz'):\n",
        "        with gzip.open(file, 'rb') as f:\n",
        "            graph.parse(file=f, format='nt')\n",
        "    else:\n",
        "        graph.parse(file, format=rdf.util.guess_format(file))\n",
        "\n",
        "    # -- Collect all node and relation labels\n",
        "    nodes = set()\n",
        "    relations = Counter()\n",
        "\n",
        "    for s, p, o in graph:\n",
        "        nodes.add(str(s))\n",
        "        nodes.add(str(o))\n",
        "        relations[str(p)] += 1\n",
        "\n",
        "    i2n = list(nodes) # maps indices to labels\n",
        "    n2i = {n:i for i, n in enumerate(i2n)} # maps labels to indices\n",
        "\n",
        "\n",
        "    i2r =list(relations.keys())\n",
        "    r2i = {r: i for i, r in enumerate(i2r)}\n",
        "\n",
        "    edges = {}\n",
        "\n",
        "    # -- Collect all edges into a list of triples\n",
        "    #    (only storing integer indices)\n",
        "    triples = [(n2i[str(s)], r2i[str(p)], n2i[str(o)]) for s, p, o in graph]\n",
        "\n",
        "\n",
        "    return triples, (n2i, i2n), (r2i, i2r)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrWaWx-n8LqR",
        "colab_type": "text"
      },
      "source": [
        "# Pytorch crash course\n",
        "\n",
        "We'll start with some basic aspects of pytorch. Pytorch is built around vectors, matrices and their higher-dimensional equivalents _tensors_. Here's how to create and manipulate them:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0m9g1zl4_Im",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make a matrix filled with (normally distributed) random numbers\n",
        "torch.randn(3, 4)\n",
        "\n",
        "# For higher-dimensional matrices (aka tensors), just add more dimensions:\n",
        "# torch.randn(3, 4, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQlPTH7N5RNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make some more\n",
        "a, b = torch.randn(3, 4), torch.rand(3, 4)\n",
        "# note the different dimensions\n",
        "c = torch.randn(4, 3)\n",
        "\n",
        "# addition, multiplication, etc are all element-wise\n",
        "summed = a + b\n",
        "mult = a * b\n",
        "power = a ** 2\n",
        "sine = torch.sin(a)\n",
        "\n",
        "# _matrix_ multiplication is done through torch.mm\n",
        "mmult = torch.mm(a, c)\n",
        "\n",
        "# Note that the following lines would both give an error. Why?\n",
        "# mult = a * c\n",
        "# torch.mm(a, b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZaZlfBZ5hFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Indexing and slicing\n",
        "print(a)\n",
        "\n",
        "print()\n",
        "print('element at the first row and third column of a:', a[0, 2]) # note: zero-indexing\n",
        "print('   first row of a:', a[0, :])\n",
        "print('first column of a:', a[:, 0])\n",
        "\n",
        "# You can also use basic slicing syntax; i:j refers to the range from i to j\n",
        "# (more precisely, i is the first element included, j is the first element \n",
        "#  excluded)\n",
        "print('middle two elements of each row of a:\\n', a[:, 1:3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4dVVE_f9HRh",
        "colab_type": "text"
      },
      "source": [
        "## Backpropagation\n",
        "\n",
        "All learning in deep learning is done through some form of _gradient descent_. To compute the gradient for some operation automatically, pytorch provides the autograd package. Autograd allows you to keep track of all the operations you perform to get to some final value (a scalar) so that you can then compute the gradient automatically, through the backpropagation algorithm. Here's how it works:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzD4GnmF-EAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "# First we wrap the a and b matrices in a Variable object. This ensures that \n",
        "# pytorch remembers how the variable was created\n",
        "\n",
        "\n",
        "av = Variable(a, requires_grad=True) # tell pytorch to remember the gradient for this one\n",
        "bv = Variable(b)\n",
        "\n",
        "# perform some operations that result in a single scalar 'out'\n",
        "multv = av * bv\n",
        "out = (multv ** 2).sum()\n",
        "\n",
        "# execute the backpropagation algorithm: compute all required gradient of 'out'\n",
        "# with respect to the variables in the operation\n",
        "out.backward()\n",
        "\n",
        "# show the gradient of out with respect to a\n",
        "print(av.grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqy4VeHsAoZE",
        "colab_type": "text"
      },
      "source": [
        "We can now build a classic multilayer perceptron very easily. All we need to do is execute the forward pass (from input to output), compute the loss (how close the output is to the target) and backpropagate the loss.\n",
        "\n",
        "We'll build a neural net for a 3D input vector $(x_1, x_2, x_3)$ to a single target value $t = {x_1} + {x_2} + x_1$. We'll give the network a hidden layer with 6 units and a ReLU activation. We'll call the weight matrices of the first and second layers w and v. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M82yFhgx-INY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The data (just one example)\n",
        "x = torch.randn(3)\n",
        "t = x[0] + x[1] + x[2]\n",
        "\n",
        "# wrap in variables (no gradient required over these)\n",
        "x = Variable(x)\n",
        "t = Variable(t)\n",
        "\n",
        "# initialize weights\n",
        "w = Variable(torch.randn(6, 3), requires_grad=True)\n",
        "v = Variable(torch.randn(1, 6), requires_grad=True)\n",
        "\n",
        "# execute the network\n",
        "h = torch.mv(w, x)   # matrix-vector multiplication\n",
        "h = torch.relu(h)    # activation\n",
        "y = torch.mv(v, h)   # second layer\n",
        "\n",
        "# compute the loss (squared error loss)\n",
        "loss = (t - y) ** 2\n",
        "\n",
        "# backpropagate\n",
        "loss.backward()\n",
        "\n",
        "# gradient on the w parameters:\n",
        "print(w.grad)\n",
        "\n",
        "# one learning step\n",
        "w.data = w.data - 0.001 * w.grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORN3fywCDkIE",
        "colab_type": "text"
      },
      "source": [
        "To make learning easier, pytorch provides some tools that perform these steps for us. The first is the _optimizer_. This is an object that performs the gradient step for us. The basic gradient step shown above is performed by the SGD optimizer, but many slightly more clever optimizers are available. Let's try out the Adam optimizer, which is a very popular choice.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-NnvVqBC4wy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim import Adam, SGD\n",
        "\n",
        "# reset the parameters, bigger hidden layer\n",
        "w = Variable(torch.randn(6, 3), requires_grad=True)\n",
        "v = Variable(torch.randn(1, 6), requires_grad=True)\n",
        "\n",
        "# we initialize the optimizer by telling it the learning rate and which parameters \n",
        "# to update\n",
        "optimizer = Adam(lr=0.0001, params=[w, v])\n",
        "\n",
        "# next, we write a training loop, showing the network new data each iteration\n",
        "# (randomly generated every time)\n",
        "\n",
        "for i in range(5000):\n",
        "  x = torch.randn(3)\n",
        "  t = x[0] + x[1] + x[2]\n",
        "\n",
        "  x, t = Variable(x), Variable(t)\n",
        "  \n",
        "  # clear all existing gradients\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  # compute the network\n",
        "  # execute the network\n",
        "  h = torch.mv(w, x)   # matrix-vector multiplication\n",
        "  h = torch.relu(h)    # activation\n",
        "  y = torch.mv(v, h)   # second layer\n",
        "\n",
        "  # compute the loss (squared error loss)\n",
        "  loss = (t - y) ** 2\n",
        "\n",
        "  # backpropagate\n",
        "  loss.backward()\n",
        "  \n",
        "  if i % 250 == 0:\n",
        "    print('iteration {}, loss {:.4}'.format(i, loss.item()))\n",
        "    # print(w)\n",
        "    \n",
        "  # take a gradient descent step\n",
        "  optimizer.step()\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep8INzV3MVM5",
        "colab_type": "text"
      },
      "source": [
        "## Neural network modules\n",
        "\n",
        "The second pytorch package that we'll look at is ```torch.nn```. This provides full featured _modules_ implementing the basic neural network layers. The fully connected layer we've used above is called linear. It takes care of initializing and remembering the parameters (including a bias).\n",
        "\n",
        "All modules in the nn package expect the data in _batches_ of multiple instance. The model is computed in parallel for all instances in the batch. This makes things much quicker on parallel hardware and helps to stabilize learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wFwLwtIE38b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "batch_size = 250\n",
        "\n",
        "# Create the two layers\n",
        "one = nn.Linear(3, 6)    \n",
        "two = nn.Linear(6, 1)\n",
        "\n",
        "# you can just apply the layers by calling, for instance, one(x), but it's \n",
        "# helpful to combine them into a module that does all this. We can create our \n",
        "# own module object of we have a complex model, but if our \n",
        "# model is just a single chain of layers, with the output of each becoming the \n",
        "# input of the next, we can use the Sequential model\n",
        "\n",
        "model = nn.Sequential(\n",
        "    one, \n",
        "    nn.ReLU(),\n",
        "    two\n",
        ")\n",
        "\n",
        "# we initialize the optimizer by telling it the learning rate and which parameters \n",
        "# to update\n",
        "optimizer = Adam(lr=0.5, params=model.parameters())\n",
        "\n",
        "# next, we write a training loop, showing the network new data each iteration\n",
        "# (randomly generated every time)\n",
        "\n",
        "for i in range(0, 50000, batch_size):\n",
        "  x = torch.randn(batch_size, 3)\n",
        "  t = x[:, 0] + x[:, 1] + x[:, 2]\n",
        "\n",
        "  x, t = Variable(x), Variable(t)\n",
        "  \n",
        "  # clear all existing gradients\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  # compute the network\n",
        "  y = model(x)\n",
        "  \n",
        "  # compute the loss (squared error loss)\n",
        "  loss = ((t - y[:, 0]) ** 2).sum() # we're using batches now, so we should sum the \n",
        "                                    # loss over all elements in the batch\n",
        "\n",
        "  # backpropagate\n",
        "  loss.backward()\n",
        "  \n",
        "  if i % 2500 == 0:\n",
        "    print('iteration {}, loss {:.4}'.format(i, loss.item()))\n",
        "    # print(w)\n",
        "    \n",
        "  # take a gradient descent step\n",
        "  optimizer.step()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klbW-YLNRd0w",
        "colab_type": "text"
      },
      "source": [
        "This may seem like a lot of hassle, just to train a simple feedforward net. Something that in other frameworks (like keras or sklearn) can be done with a few lines of code. \n",
        "\n",
        "That's true, but the benefit is that we have a very flexible setup. We can compute any sequence of linear algebra operations on our data, and so long as it results in a single loss value, we can call ```loss.backward()``` to compute the gradient and optimize the parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJMDNYrgSKVl",
        "colab_type": "text"
      },
      "source": [
        "# A simple embedding model: matrix factorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyCWQqR0SNvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "num_users, num_items = 20, 40\n",
        "\n",
        "likes = generate_likes(num_users, num_items)\n",
        "num_likes = likes.size(0)\n",
        "\n",
        "# model parameters: embeddings for the users and for the items\n",
        "users = Variable(torch.randn(num_users, 2), requires_grad=True)\n",
        "items = Variable(torch.randn(num_items, 2), requires_grad=True)\n",
        "\n",
        "opt = Adam(lr = 0.0005, params=[users, items])\n",
        "\n",
        "for it in range(5000):\n",
        "  opt.zero_grad()\n",
        "  \n",
        "  # pick a random positive pair\n",
        "  u, i = likes[random.randint(0, num_likes-1), :]\n",
        "  \n",
        "  # get the embeddings \n",
        "  user, item = users[u, :], items[i, :]\n",
        "  \n",
        "  # compute the dot product\n",
        "  pdot = torch.dot(user, item)\n",
        "  \n",
        "  # pick a random negative pair\n",
        "  u, i = random.randint(0, num_users-1), random.randint(0, num_items-1)\n",
        "  \n",
        "  # get the embeddings \n",
        "  user, item = users[u, :], items[i, :] #\n",
        "  \n",
        "  # compute the dot product\n",
        "  ndot = torch.dot(user, item)\n",
        "  \n",
        "  loss = ndot - pdot \n",
        "  \n",
        "  loss.backward()\n",
        "  \n",
        "  opt.step()\n",
        "  \n",
        "  if it % 250 == 0:\n",
        "    print(it, loss.item())\n",
        "\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6snyHJ6222d",
        "colab_type": "text"
      },
      "source": [
        "It's a little difficult to see whether the loss actually goes down. We'll fix that later, when we move to knowledge graphs, by processing the data in batches (so the loss is averaged over multiple examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0psTqntna2U",
        "colab_type": "text"
      },
      "source": [
        "# Knowledge graphs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_ZCuLi4NRuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install rdflib wget"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ynQPz6Mnoif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import rdflib as rdf\n",
        "import wget, gzip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syzCYF6Bp_ev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load a small example knowledge graph (the AIFB dataset)\n",
        "# -- this function loads the graph and maps all urls (nodes and relations) to \n",
        "#    unique integers, that we can work with in a machine learning context.\n",
        "# -- triples is a list of integer triples. n2i maps nodes to integer indices and \n",
        "#    similar for the others\n",
        "triples, (n2i, i2n), (r2i, i2r) = load_aifb()\n",
        "\n",
        "num_nodes, num_rels, num_links = len(i2n), len(i2r), len(triples)\n",
        "\n",
        "print(num_nodes, num_rels, num_links)\n",
        "triples[:10]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w15FSecUzMzH",
        "colab_type": "text"
      },
      "source": [
        "# RESCAL\n",
        "\n",
        "_NB: The proper RESCAL algorithm has some more features than this. We're just illustrating the RESCAL score function._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtL0Lqezrr3g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "# set up the model parameters\n",
        "\n",
        "k = 16 # embedding dimension\n",
        "\n",
        "# -- node embeddings\n",
        "nodes = nn.Parameter(torch.randn(num_nodes, k))   # single k-vector per node\n",
        "# -- relation embeddings\n",
        "rels  = nn.Parameter(torch.randn(num_rels, k, k)) # k-by-k matrix per relation\n",
        "\n",
        "# (nn.Parameter is basically the same as Variable, but gradients are turned on\n",
        "#  by default, together with some other helpful features.)\n",
        "\n",
        "opt = Adam(lr = 0.001, params=[nodes, rels])\n",
        "\n",
        "for it in range(5000):\n",
        "  opt.zero_grad()\n",
        "  \n",
        "  # pick a random positive pair\n",
        "  s, p, o = random.choice(triples)\n",
        "  \n",
        "  # get the embeddings, compute the score\n",
        "  sub, pred, obj = nodes[s, :], rels[p, :, :], nodes[o, :]\n",
        "  pscore = sub.dot(torch.mv(pred, obj))\n",
        "  \n",
        "  # pick a random negative pair \n",
        "  # -- we do this by choosing an existing triple and corrupting either the left \n",
        "  #    or right node\n",
        "  s, p, o = random.choice(triples)\n",
        "  if random.choice([True, False]):\n",
        "    s = random.randint(0, num_nodes - 1)\n",
        "  else:\n",
        "    o = random.randint(0, num_nodes - 1)\n",
        "  \n",
        "  # get the embeddings \n",
        "  sub, pred, obj = nodes[s, :], rels[p, :, :], nodes[o, :]\n",
        "  # compute the bilinear product, compute the score\n",
        "  nscore = sub.dot(torch.mv(pred, obj))\n",
        "  \n",
        "  loss = nscore - pscore\n",
        "  \n",
        "  loss.backward()\n",
        "  \n",
        "  opt.step()\n",
        "  \n",
        "  if it % 250 == 0:\n",
        "    print(it, loss.item())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFx_dx2az0by",
        "colab_type": "text"
      },
      "source": [
        "# DistMult\n",
        "\n",
        "The RESCAL score function works pretty well, but the relation embeddings tend to be overparametrized, compared to those of the nodes. The _DistMult_ score functionfixes this by forcing the relation embeddings to be diagonal (i.e. zero everywhere but the diagonal), so that the relations are embedded in a vector as well.\n",
        "\n",
        "This means that the score function for a triple $(s, p, o)$ with embeddings $e_s$, $e_p$, $e_o$ becomes:\n",
        "\n",
        "$$\\text{score} = {e_s}^T \\text{diag}(e_r)e_o = \\text{sum}(e_s \\otimes e_r \\otimes e_o) $$\n",
        "\n",
        "Where $\\otimes$ represents element-wise multiplication.\n",
        "\n",
        "Below, we'll implement DistMult. We'll also add batching to the code, so that we process multiple randomly selected triples per iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rLV1AvjzVCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import sigmoid, log\n",
        "\n",
        "# set up the model parameters\n",
        "\n",
        "batch_size = 64\n",
        "k = 16 # embedding dimension\n",
        "\n",
        "# -- node embeddings\n",
        "nodes = nn.Parameter(torch.randn(num_nodes, k))   # single k-vector per node\n",
        "# -- relation embeddings\n",
        "rels  = nn.Parameter(torch.randn(num_rels, k)) # k-by-k matrix per relation\n",
        "\n",
        "# (nn.Parameter is basically the same as Variable, but gradients are turned on\n",
        "#  by default, together with some other helpful features.)\n",
        "\n",
        "opt = Adam(lr = 0.00001, params=[nodes, rels])\n",
        "\n",
        "for it in range(200_000): # decent looking results at abt 100k to 500k\n",
        "  opt.zero_grad()\n",
        "  \n",
        "  # select a batch of positive pairs\n",
        "  positives = random.sample(triples, batch_size)\n",
        "  # -- split them into indices for s, p and o\n",
        "  s, p, o = [s for s, _, _ in positives], [p for _, p, _ in positives], [o for _, _, o in positives]\n",
        "  \n",
        "  # get the embeddings, compute the score\n",
        "  # -- pytorch allows us to use a list of integers to select multiple rows\n",
        "  sub, pred, obj = nodes[s, :], rels[p, :], nodes[o, :]\n",
        "  \n",
        "  pscore = (sub * pred * obj).sum(dim=1) # we sum over dimension 1, so we get a \n",
        "                                         # score for each instancein the batch \n",
        "  \n",
        "  # pick a random negative pair \n",
        "  # -- we do this by choosing an existing triple and corrupting either the left \n",
        "  #    or right node\n",
        "  \n",
        "  # -- sample some triples\n",
        "  negatives = random.sample(triples, batch_size)\n",
        "  # -- split them into indices for s, p and o\n",
        "  s, p, o = [s for s, _, _ in positives], [p for _, p, _ in positives], [o for _, _, o in positives]  \n",
        "  \n",
        "  s, p, o = torch.tensor(s), torch.tensor(p), torch.tensor(o)\n",
        "  \n",
        "  indices = torch.rand(batch_size) > 0.5 # vector of random bits\n",
        "  \n",
        "  # vector of random node indices\n",
        "  nwnodes = (torch.rand(batch_size) * num_nodes).floor().long()\n",
        "\n",
        "  # corrupt sub at the chosen indices, corrupt obj at the other indices\n",
        "  s[ indices] = nwnodes[ indices]\n",
        "  o[~indices] = nwnodes[~indices]\n",
        "  \n",
        "  # get the embeddings \n",
        "  sub, pred, obj = nodes[s, :], rels[p, :], nodes[o, :]\n",
        "\n",
        "  # compute the  the score\n",
        "  nscore = (sub * pred * obj).sum(dim=1)\n",
        "  \n",
        "  # plain loss\n",
        "  # loss = nscore.sum() - pscore.sum()\n",
        "  \n",
        "  # log loss\n",
        "  loss = log(sigmoid(nscore)).sum() - log(sigmoid(pscore)).sum()\n",
        "  \n",
        "  loss.backward()\n",
        "  \n",
        "  opt.step()\n",
        "  \n",
        "  if it % 20_000 == 0:\n",
        "    print(it, loss.item())\n",
        "    \n",
        "# NB: This one may take a while ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeZb7Odn9Bds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pick a random triple, and print its labels\n",
        "\n",
        "i = random.randint(0, len(triples)-1)\n",
        "s, p, o = triples[i]\n",
        "\n",
        "print('using triple {}:'.format(i), i2n[s], i2r[p], i2n[o])\n",
        "\n",
        "scores = []\n",
        "\n",
        "\n",
        "sub = nodes[s, :]\n",
        "pred = rels[p, :]\n",
        "\n",
        "for o in range(num_nodes):\n",
        "\n",
        "  obj = nodes[o, :]\n",
        "  \n",
        "  score = (sub * pred * obj).sum().item()\n",
        "  \n",
        "  scores.append((o, score))\n",
        "\n",
        "# sort by score\n",
        "scores.sort(key = lambda x : - x[1])\n",
        "\n",
        "# print top 10\n",
        "for o, score in scores[:10]:\n",
        "  print('{},\\t {:.4}'.format(i2n[o], score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTzFWIsoIFwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}